{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpKVFlfIGbNimpYrbZOLCI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praharshithapanthangi/CREDIT_RISK_ANALYSIS_IDP/blob/main/IDP_PROJECT_CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFZB7GT0CQ8A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import missingno as msno\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "from pandas_profiling import ProfileReport\n",
        "from pathlib import Path\n",
        "from scipy.stats import probplot, chi2_contingency, chi2\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, roc_curve, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "import scikitplot as skplt\n",
        "from yellowbrick.model_selection import FeatureImportances\n",
        "import scipy.stats as stats\n",
        "import joblib\n",
        "import os\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cc_data_full_data = pd.read_csv('/content/sample_data/application_record.csv')\n",
        "credit_status = pd.read_csv('/content/sample_data/credit_record.csv')"
      ],
      "metadata": {
        "id": "eR0xnf6ZCg6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "begin_month=pd.DataFrame(credit_status.groupby(['ID'])['MONTHS_BALANCE'].agg(min))\n",
        "begin_month=begin_month.rename(columns={'MONTHS_BALANCE':'Account age'})\n",
        "cc_data_full_data=pd.merge(cc_data_full_data,begin_month,how='left',on='ID')\n",
        "credit_status['dep_value'] = None\n",
        "credit_status['dep_value'][credit_status['STATUS'] =='2']='Yes'\n",
        "credit_status['dep_value'][credit_status['STATUS'] =='3']='Yes'\n",
        "credit_status['dep_value'][credit_status['STATUS'] =='4']='Yes'\n",
        "credit_status['dep_value'][credit_status['STATUS'] =='5']='Yes'\n",
        "cpunt=credit_status.groupby('ID').count()\n",
        "cpunt['dep_value'][cpunt['dep_value'] > 0]='Yes'\n",
        "cpunt['dep_value'][cpunt['dep_value'] == 0]='No'\n",
        "cpunt = cpunt[['dep_value']]\n",
        "cc_data_full_data = pd.merge(cc_data_full_data,cpunt,how='inner',on='ID')\n",
        "cc_data_full_data['Is high risk']=cc_data_full_data['dep_value']\n",
        "cc_data_full_data.loc[cc_data_full_data['Is high risk']=='Yes','Is high risk']=1\n",
        "cc_data_full_data.loc[cc_data_full_data['Is high risk']=='No','Is high risk']=0\n",
        "cc_data_full_data.drop('dep_value',axis=1,inplace=True)\n",
        "pd.options.mode.chained_assignment = None # hide warning SettingWithCopyWarning"
      ],
      "metadata": {
        "id": "sDn7RVGsCg23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename the features to a more readable feature names\n",
        "cc_data_full_data = cc_data_full_data.rename(columns={\n",
        "    'CODE_GENDER':'Gender',\n",
        "    'FLAG_OWN_CAR':'Has a car',\n",
        "    'FLAG_OWN_REALTY':'Has a property',\n",
        "    'CNT_CHILDREN':'Children count',\n",
        "    'AMT_INCOME_TOTAL':'Income',\n",
        "    'NAME_INCOME_TYPE':'Employment status',\n",
        "    'NAME_EDUCATION_TYPE':'Education level',\n",
        "    'NAME_FAMILY_STATUS':'Marital status',\n",
        "    'NAME_HOUSING_TYPE':'Dwelling',\n",
        "    'DAYS_BIRTH':'Age',\n",
        "    'DAYS_EMPLOYED': 'Employment length',\n",
        "    'FLAG_MOBIL': 'Has a mobile phone',\n",
        "    'FLAG_WORK_PHONE': 'Has a work phone',\n",
        "    'FLAG_PHONE': 'Has a phone',\n",
        "    'FLAG_EMAIL': 'Has an email',\n",
        "    'OCCUPATION_TYPE': 'Job title',\n",
        "    'CNT_FAM_MEMBERS': 'Family member count',\n",
        "    'Account age': 'Account a"
      ],
      "metadata": {
        "id": "GIT3dKliCg0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and test\n",
        "def data_split(df, test_size):\n",
        "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
        "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "3Lw4VoRcCgyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_train_original, cc_test_original = data_split(cc_data_full_data, 0.2)\n"
      ],
      "metadata": {
        "id": "qpH3k40TCgwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_train_original.shape"
      ],
      "metadata": {
        "id": "fBXsXW_2Cgtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_train_original.to_csv('/content/sample_data/train.csv',index=False)"
      ],
      "metadata": {
        "id": "JW3zSr6DCgrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a copy of the dataset so that the original stays untouched\n",
        "cc_train_copy = cc_train_original.copy()\n",
        "cc_test_copy = cc_test_original.copy()"
      ],
      "metadata": {
        "id": "O2eOXlOcCgof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_data_full_data.head()"
      ],
      "metadata": {
        "id": "BJmjpa3wCgmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_data_full_data.info()"
      ],
      "metadata": {
        "id": "wI55GGwwCgj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_data_full_data.describe()"
      ],
      "metadata": {
        "id": "BYhzJ2ghCghQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msno.matrix(cc_data_full_data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pe3-T16FCgfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msno.bar(cc_data_full_data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qEi2irqwCgcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that will return the value count and frequency of each observation within a feature\n",
        "def value_cnt_norm_cal(df,feature):\n",
        "    ftr_value_cnt = df[feature].value_counts()\n",
        "    ftr_value_cnt_norm = df[feature].value_counts(normalize=True) * 100\n",
        "    ftr_value_cnt_concat = pd.concat([ftr_value_cnt, ftr_value_cnt_norm], axis=1)\n",
        "    ftr_value_cnt_concat.columns = ['Count', 'Frequency (%)']\n",
        "    return ftr_value_cnt_concat"
      ],
      "metadata": {
        "id": "MOZAd7pcCgab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create display general information about the feature\n",
        "def gen_info_feat(df,feature):\n",
        "    match feature:\n",
        "        case 'Age':\n",
        "            # change the feature to be express in positive numbers days\n",
        "            print('Description:\\n{}'.format((np.abs(df[feature])/365.25).describe()))\n",
        "            print('*'*50)\n",
        "            print('Object type:{}'.format(df[feature].dtype))\n",
        "        case 'Employment length':\n",
        "            # select only the rows where the rows are negative to ignore whose who have retired or unemployed\n",
        "            employment_len_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] < 0]\n",
        "            employment_len_no_ret_yrs = np.abs(employment_len_no_ret)/365.25\n",
        "            print('Description:\\n{}'.format((employment_len_no_ret_yrs).describe()))\n",
        "            print('*'*50)\n",
        "            print('Object type:{}'.format(employment_len_no_ret.dtype))\n",
        "        case 'Account age':\n",
        "            # change the account age to a positive number of months\n",
        "            print('Description:\\n{}'.format((np.abs(df[feature])).describe()))\n",
        "            print('*'*50)\n",
        "            print('Object type:{}'.format(df[feature].dtype))\n",
        "        case _:\n",
        "            print('Description:\\n{}'.format(df[feature].describe()))\n",
        "            print('*'*50)\n",
        "            print('Object type:\\n{}'.format(df[feature].dtype))\n",
        "            print('*'*50)\n",
        "            value_cnt = value_cnt_norm_cal(df,feature)\n",
        "            print('Value count:\\n{}'.format(value_cnt))"
      ],
      "metadata": {
        "id": "rwsQB3MRCgX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create a pie chart plot\n",
        "def create_pie_plot(df,feature):\n",
        "    match feature:\n",
        "        case 'Dwelling' | 'Education level':\n",
        "            ratio_size = value_cnt_norm_cal(df, feature)\n",
        "            ratio_size_len = len(ratio_size.index)\n",
        "            ratio_list = []\n",
        "            for i in range(ratio_size_len):\n",
        "                ratio_list.append(ratio_size.iloc[i]['Frequency (%)'])\n",
        "            fig, ax = plt.subplots(figsize=(8,8))\n",
        "            # %1.2f%% display decimals in the pie chart with 2 decimal places\n",
        "            plt.pie(ratio_list, startangle=90, wedgeprops={'edgecolor' :'black'})\n",
        "            plt.title('Pie chart of {}'.format(feature))\n",
        "            plt.legend(loc='best',labels=ratio_size.index)\n",
        "            plt.axis('equal')\n",
        "            return plt.show()\n",
        "        case _:\n",
        "            ratio_size = value_cnt_norm_cal(df, feature)\n",
        "            ratio_size_len = len(ratio_size.index)\n",
        "            ratio_list = []\n",
        "            for i in range(ratio_size_len):\n",
        "                ratio_list.append(ratio_size.iloc[i]['Frequency (%)'])\n",
        "            fig, ax = plt.subplots(figsize=(8,8))\n",
        "            # %1.2f%% display decimals in the pie chart with 2 decimal places\n",
        "            plt.pie(ratio_list, labels=ratio_size.index, autopct='%1.2f%%', startangle=90, wedgeprops={'edgecolor' :'black'})\n",
        "            plt.title('Pie chart of {}'.format(feature))\n",
        "            plt.legend(loc='best')\n",
        "            plt.axis('equal')\n",
        "            return plt.show()"
      ],
      "metadata": {
        "id": "lfW1HKCvCgVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create a bar chart plot\n",
        "def create_bar_plot(df,feature):\n",
        "    match feature:\n",
        "        case 'Marital status' | 'Dwelling' | 'Job title' | 'Employment status' | 'Education level':\n",
        "            fig, ax = plt.subplots(figsize=(6,10))\n",
        "            sns.barplot(x=value_cnt_norm_cal(df,feature).index,y=value_cnt_norm_cal(df,feature).values[:,0])\n",
        "            ax.set_xticklabels(labels=value_cnt_norm_cal(df,feature).index,rotation=45,ha='right')\n",
        "            plt.xlabel('{}'.format(feature))\n",
        "            plt.ylabel('Count')\n",
        "            plt.title('{} count'.format(feature))\n",
        "            return plt.show()\n",
        "        case _:\n",
        "            fig, ax = plt.subplots(figsize=(6,10))\n",
        "            sns.barplot(x=value_cnt_norm_cal(df,feature).index,y=value_cnt_norm_cal(df,feature).values[:,0])\n",
        "            plt.xlabel('{}'.format(feature))\n",
        "            plt.ylabel('Count')\n",
        "            plt.title('{} count'.format(feature))\n",
        "            return plt.show()"
      ],
      "metadata": {
        "id": "ZE3xQBN-CgTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create a box plot\n",
        "def create_box_plot(df,feature):\n",
        "    match feature:\n",
        "        case 'Age':\n",
        "            fig, ax = plt.subplots(figsize=(2,8))\n",
        "            # change the feature to be express in positive numbers days\n",
        "            sns.boxplot(y=np.abs(df[feature])/365.25)\n",
        "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
        "            return plt.show()\n",
        "        case 'Children count':\n",
        "            fig, ax = plt.subplots(figsize=(2,8))\n",
        "            sns.boxplot(y=df[feature])\n",
        "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
        "            plt.yticks(np.arange(0,df[feature].max(),1))\n",
        "            return plt.show()\n",
        "        case 'Employment length':\n",
        "            fig, ax = plt.subplots(figsize=(2,8))\n",
        "            employment_len_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] < 0]\n",
        "            # employement length in days is a negative number so we need to change it to positive and change it to days\n",
        "            employment_len_no_ret_yrs = np.abs(employment_len_no_ret)/365.25\n",
        "            sns.boxplot(y=employment_len_no_ret_yrs)\n",
        "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
        "            plt.yticks(np.arange(0,employment_len_no_ret_yrs.max(),2))\n",
        "            return plt.show()\n",
        "        case 'Income':\n",
        "            fig, ax = plt.subplots(figsize=(2,8))\n",
        "            sns.boxplot(y=df[feature])\n",
        "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
        "            # suppress scientific notation\n",
        "            ax.get_yaxis().set_major_formatter(\n",
        "                matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "            return plt.show()\n",
        "        case 'Account age':\n",
        "            fig, ax = plt.subplots(figsize=(2,8))\n",
        "            sns.boxplot(y=np.abs(df[feature]))\n",
        "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
        "            return plt.show()\n",
        "        case _:\n",
        "            fig, ax = plt.subplots(figsize=(2,8))\n",
        "            sns.boxplot(y=df[feature])\n",
        "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
        "            return plt.show()"
      ],
      "metadata": {
        "id": "8dwxDqnuDFst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create a histogram plot\n",
        "def create_hist_plot(df,feature, the_bins=50):\n",
        "    match feature:\n",
        "        case 'Age':\n",
        "            fig, ax = plt.subplots(figsize=(18,10))\n",
        "            # change the feature to be express in positive numbers days\n",
        "            sns.histplot(np.abs(df[feature])/365.25,bins=the_bins,kde=True)\n",
        "            plt.title('{} distribution'.format(feature))\n",
        "            return plt.show()\n",
        "        case 'Income':\n",
        "            fig, ax = plt.subplots(figsize=(18,10))\n",
        "            sns.histplot(df[feature],bins=the_bins,kde=True)\n",
        "            # suppress scientific notation\n",
        "            ax.get_xaxis().set_major_formatter(\n",
        "                matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "            plt.title('{} distribution'.format(feature))\n",
        "            return plt.show()\n",
        "        case 'Employment length':\n",
        "            employment_len_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] < 0]\n",
        "            # change the feature to be express in positive numbers days\n",
        "            employment_len_no_ret_yrs = np.abs(employment_len_no_ret)/365.25\n",
        "            fig, ax = plt.subplots(figsize=(18,10))\n",
        "            sns.histplot(employment_len_no_ret_yrs,bins=the_bins,kde=True)\n",
        "            plt.title('{} distribution'.format(feature))\n",
        "            return plt.show()\n",
        "        case 'Account age':\n",
        "            fig, ax = plt.subplots(figsize=(18,10))\n",
        "            sns.histplot(np.abs(df[feature]),bins=the_bins,kde=True)\n",
        "            plt.title('{} distribution'.format(feature))\n",
        "            return plt.show()\n",
        "        case _:\n",
        "            fig, ax = plt.subplots(figsize=(18,10))\n",
        "            sns.histplot(df[feature],bins=the_bins,kde=True)\n",
        "            plt.title('{} distribution'.format(feature))\n",
        "            return plt.show()"
      ],
      "metadata": {
        "id": "mvU0PCvTDFpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# High risk vs low risk applicants compared on a box plot\n",
        "def low_high_risk_box_plot(df,feature):\n",
        "    match feature:\n",
        "        case 'Age':\n",
        "            print(np.abs(df.groupby('Is high risk')[feature].mean()/365.25))\n",
        "            fig, ax = plt.subplots(figsize=(5,8))\n",
        "            sns.boxplot(y=np.abs(df[feature])/365.25,x=df['Is high risk'])\n",
        "            plt.xticks(ticks=[0,1],labels=['no','yes'])\n",
        "            plt.title('High risk individuals grouped by age')\n",
        "            return plt.show()\n",
        "        case 'Income':\n",
        "            print(np.abs(df.groupby('Is high risk')[feature].mean()))\n",
        "            fig, ax = plt.subplots(figsize=(5,8))\n",
        "            sns.boxplot(y=np.abs(df[feature]),x=df['Is high risk'])\n",
        "            plt.xticks(ticks=[0,1],labels=['no','yes'])\n",
        "            # suppress scientific notation\n",
        "            ax.get_yaxis().set_major_formatter(\n",
        "                matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "            plt.title('High risk individuals grouped by {}'.format(feature))\n",
        "            return plt.show()\n",
        "        case 'Employment length':\n",
        "            #checking is an applicant is high risk or not (for those who have negative employment length aka the employed ones)\n",
        "            employment_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] <0]\n",
        "            employment_no_ret_idx = employment_no_ret.index\n",
        "            employment_len_no_ret_yrs = np.abs(employment_no_ret)/365.25\n",
        "            employment_no_ret_df = cc_train_copy.iloc[employment_no_ret_idx][['Employment length','Is high risk']]\n",
        "            employment_no_ret_is_high_risk = employment_no_ret_df.groupby('Is high risk')['Employment length'].mean()\n",
        "            # compare the age of high risk individuals with the age of low risk individuals (those who are employed)\n",
        "            print(np.abs(employment_no_ret_is_high_risk)/365.25)\n",
        "            fig, ax = plt.subplots(figsize=(5,8))\n",
        "            sns.boxplot(y=employment_len_no_ret_yrs,x=df['Is high risk'])\n",
        "            plt.xticks(ticks=[0,1],labels=['no','yes'])\n",
        "            plt.title('High vs low risk individuals grouped by {}'.format(feature))\n",
        "            return plt.show()\n",
        "        case _:\n",
        "            print(np.abs(df.groupby('Is high risk')[feature].mean()))\n",
        "            fig, ax = plt.subplots(figsize=(5,8))\n",
        "            sns.boxplot(y=np.abs(df[feature]),x=df['Is high risk'])\n",
        "            plt.xticks(ticks=[0,1],labels=['no','yes'])\n",
        "            plt.title('High risk individuals grouped by {}'.format(feature))\n",
        "            return plt.show()"
      ],
      "metadata": {
        "id": "ig7vvLelDFm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# High risk vs low risk applicants compared on a bar plot\n",
        "def low_high_risk_bar_plot(df,feature):\n",
        "    is_high_risk_grp = df.groupby(feature)['Is high risk'].sum()\n",
        "    is_high_risk_grp_srt = is_high_risk_grp.sort_values(ascending=False)\n",
        "    print(dict(is_high_risk_grp_srt))\n",
        "    fig, ax = plt.subplots(figsize=(6,10))\n",
        "    sns.barplot(x=is_high_risk_grp_srt.index,y=is_high_risk_grp_srt.values)\n",
        "    ax.set_xticklabels(labels=is_high_risk_grp_srt.index,rotation=45, ha='right')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('High risk applicants count grouped by {}'.format(feature))\n",
        "    return plt.show()"
      ],
      "metadata": {
        "id": "usNJp4ODDFkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_train_copy.shape"
      ],
      "metadata": {
        "id": "oAOpul8EDQJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "gen_info_feat(cc_train_copy,'Income')"
      ],
      "metadata": {
        "id": "CFiYZb9jD-8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(cc_train_copy[cc_train_copy['Employment length'] < 0].drop(['ID','Has a mobile phone', 'Has a work phone', 'Has a phone', 'Has an email','Is high risk'],axis=1),corner=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vPgRruTmGhNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.regplot(x='Children count',y='Family member count',data=cc_train_copy,line_kws={'color': 'red'})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7qgAqiYpFGCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_age = np.abs(cc_train_copy['Age'])/365.25\n",
        "sns.jointplot(np.abs(cc_train_copy['Account age']),y_age, kind=\"hex\", height=12)\n",
        "plt.yticks(np.arange(20, y_age.max(), 5))\n",
        "plt.xticks(np.arange(0, 65, 5))\n",
        "plt.ylabel('Age')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a1K-v9r-FGAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_employ_length = np.abs(cc_train_copy[cc_train_copy['Employment length'] < 0]['Employment length'])/365.25\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "sns.scatterplot(x_employ_length,y_age,alpha=.05)\n",
        "# change the frequency of the x-axis and y-axis labels\n",
        "plt.xticks(np.arange(0, x_employ_length.max(), 2.5))\n",
        "plt.yticks(np.arange(20, y_age.max(), 5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U0eUEn7VFF-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change the datatype of target feature to int\n",
        "is_high_risk_int = cc_train_copy['Is high risk'].astype('int32')"
      ],
      "metadata": {
        "id": "KL59_aEzFPju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation analysis with heatmap, after dropping the has a mobile phone with the target feature as int\n",
        "cc_train_copy_corr_no_mobile = pd.concat([cc_train_copy.drop(['Has a mobile phone','Is high risk'], axis=1),is_high_risk_int],axis=1).corr()\n",
        "# Get the lower triangle of the correlation matrix\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.zeros_like(cc_train_copy_corr_no_mobile, dtype='bool')\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "# Set up the matplotlib figure\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "# seaborn heatmap\n",
        "sns.heatmap(cc_train_copy_corr_no_mobile, annot=True, cmap='flare',mask=mask, linewidths=.5)\n",
        "# plot the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KIGfelc4FPgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chi_func(feature):\n",
        "    # selection row with high risk\n",
        "    high_risk_ft = cc_train_copy[cc_train_copy['Is high risk'] == 1][feature]\n",
        "    high_risk_ft_ct = pd.crosstab(index=high_risk_ft, columns=['Count']).rename_axis(None, axis=1)\n",
        "    # drop the index feature name\n",
        "    high_risk_ft_ct.index.name = None\n",
        "    # observe values\n",
        "    obs = high_risk_ft_ct\n",
        "    print('Observed values:\\n')\n",
        "    print(obs)\n",
        "    print('\\n')\n",
        "    # expected values\n",
        "    print(obs.index)\n",
        "    exp = pd.DataFrame([obs['Count'].sum()/len(obs)] * len(obs.index),columns=['Count'], index=obs.index)\n",
        "    print('Expected values:\\n')\n",
        "    print(exp)\n",
        "    print('\\n')\n",
        "    # chi-square test\n",
        "    chi_squared_stat = (((obs-exp)**2)/exp).sum()\n",
        "    print('Chi-square:\\n')\n",
        "    print(chi_squared_stat[0])\n",
        "    print('\\n')\n",
        "    #critical value\n",
        "    crit = stats.chi2.ppf(q = 0.95, df = len(obs) - 1)\n",
        "    print('Critical value:\\n')\n",
        "    print(crit)\n",
        "    print('\\n')\n",
        "    # p-value\n",
        "    p_value = 1 - stats.chi2.cdf(x = chi_squared_stat, df = len(obs) - 1)\n",
        "    print('P-value:\\n')\n",
        "    print(p_value)\n",
        "    print('\\n')\n",
        "    if chi_squared_stat[0] >= crit:\n",
        "        print('Reject the null hypothesis')\n",
        "    elif chi_squared_stat[0] <= crit:\n",
        "        print('Fail to reject the null hypothesis')"
      ],
      "metadata": {
        "id": "hElFOkMdFPeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_ft = ['Gender', 'Has a car', 'Has a property', 'Employment status', 'Education level', 'Marital status', 'Dwelling', 'Job title']\n",
        "for ft in cat_ft:\n",
        "    print('\\n\\n**** {} ****\\n'.format(ft))\n",
        "    chi_func(ft)"
      ],
      "metadata": {
        "id": "KSFJsGlTFPb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,feat_with_outliers = ['Family member count','Income', 'Employment length']):\n",
        "        self.feat_with_outliers = feat_with_outliers\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "    def transform(self,df):\n",
        "        if (set(self.feat_with_outliers).issubset(df.columns)):\n",
        "            # 25% quantile\n",
        "            Q1 = df[self.feat_with_outliers].quantile(.25)\n",
        "            # 75% quantile\n",
        "            Q3 = df[self.feat_with_outliers].quantile(.75)\n",
        "            IQR = Q3 - Q1\n",
        "            # keep the data within 3 IQR\n",
        "            df = df[~((df[self.feat_with_outliers] < (Q1 - 3 * IQR)) |(df[self.feat_with_outliers] > (Q3 + 3 * IQR))).any(axis=1)]\n",
        "            return df\n",
        "        else:\n",
        "            print(\"One or more features are not in the dataframe\")\n",
        "            return df"
      ],
      "metadata": {
        "id": "tUFeQ82TFPZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DropFeatures(BaseEstimator,TransformerMixin):\n",
        "    def __init__(self,feature_to_drop = ['ID','Has a mobile phone','Children count','Job title','Account age']):\n",
        "        self.feature_to_drop = feature_to_drop\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "    def transform(self,df):\n",
        "        if (set(self.feature_to_drop).issubset(df.columns)):\n",
        "            df.drop(self.feature_to_drop,axis=1,inplace=True)\n",
        "            return df\n",
        "        else:\n",
        "            print(\"One or more features are not in the dataframe\")\n",
        "            return df"
      ],
      "metadata": {
        "id": "B2QFtcUYFc0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeConversionHandler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, feat_with_days = ['Employment length', 'Age']):\n",
        "        self.feat_with_days = feat_with_days\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        if (set(self.feat_with_days).issubset(X.columns)):\n",
        "            # convert days to absolute value\n",
        "            X[['Employment length','Age']] = np.abs(X[['Employment length','Age']])\n",
        "            return X\n",
        "        else:\n",
        "            print(\"One or more features are not in the dataframe\")\n",
        "            return X"
      ],
      "metadata": {
        "id": "8614hsJwFcxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RetireeHandler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit(self, df):\n",
        "        return self\n",
        "    def transform(self, df):\n",
        "        if 'Employment length' in df.columns:\n",
        "            # select rows with employment length is 365243 which corresponds to retirees\n",
        "            df_ret_idx = df['Employment length'][df['Employment length'] == 365243].index\n",
        "            # change 365243 to 0\n",
        "            df.loc[df_ret_idx,'Employment length'] = 0\n",
        "            return df\n",
        "        else:\n",
        "            print(\"Employment length is not in the dataframe\")\n",
        "            return df"
      ],
      "metadata": {
        "id": "H7oY9EbxFcu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkewnessHandler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,feat_with_skewness=['Income','Age']):\n",
        "        self.feat_with_skewness = feat_with_skewness\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "    def transform(self,df):\n",
        "        if (set(self.feat_with_skewness).issubset(df.columns)):\n",
        "            # Handle skewness with cubic root transformation\n",
        "            df[self.feat_with_skewness] = np.cbrt(df[self.feat_with_skewness])\n",
        "            return df\n",
        "        else:\n",
        "            print(\"One or more features are not in the dataframe\")\n",
        "            return df"
      ],
      "metadata": {
        "id": "48F3lDNrFcs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinningNumToYN(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,feat_with_num_enc=['Has a work phone','Has a phone','Has an email']):\n",
        "        self.feat_with_num_enc = feat_with_num_enc\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "    def transform(self,df):\n",
        "        if (set(self.feat_with_num_enc).issubset(df.columns)):\n",
        "            # Change 0 to N and 1 to Y for all the features in feat_with_num_enc\n",
        "            for ft in self.feat_with_num_enc:\n",
        "                df[ft] = df[ft].map({1:'Y',0:'N'})\n",
        "            return df\n",
        "        else:\n",
        "            print(\"One or more features are not in the dataframe\")\n",
        "            return df"
      ],
      "metadata": {
        "id": "JKBm-BTdFcqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotWithFeatNames(BaseEstimator,TransformerMixin):\n",
        "    def __init__(self,one_hot_enc_ft = ['Gender', 'Marital status', 'Dwelling', 'Employment status', 'Has a car', 'Has a property', 'Has a work phone', 'Has a phone', 'Has an email']):\n",
        "        self.one_hot_enc_ft = one_hot_enc_ft\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "    def transform(self,df):\n",
        "        if (set(self.one_hot_enc_ft).issubset(df.columns)):\n",
        "            # function to one hot encode the features in one_hot_enc_ft\n",
        "            def one_hot_enc(df,one_hot_enc_ft):\n",
        "                one_hot_enc = OneHotEncoder()\n",
        "                one_hot_enc.fit(df[one_hot_enc_ft])\n",
        "                # get the result of the one hot encoding columns names\n",
        "                feat_names_one_hot_enc = one_hot_enc.get_feature_names_out(one_hot_enc_ft)\n",
        "                # change the array of the one hot encoding to a dataframe with the column names\n",
        "                df = pd.DataFrame(one_hot_enc.transform(df[self.one_hot_enc_ft]).toarray(),columns=feat_names_one_hot_enc,index=df.index)\n",
        "                return df\n",
        "            # function to concatenat the one hot encoded features with the rest of features that were not encoded\n",
        "            def concat_with_rest(df,one_hot_enc_df,one_hot_enc_ft):\n",
        "                # get the rest of the features\n",
        "                rest_of_features = [ft for ft in df.columns if ft not in one_hot_enc_ft]\n",
        "                # concatenate the rest of the features with the one hot encoded features\n",
        "                df_concat = pd.concat([one_hot_enc_df, df[rest_of_features]],axis=1)\n",
        "                return df_concat\n",
        "            # one hot encoded dataframe\n",
        "            one_hot_enc_df = one_hot_enc(df,self.one_hot_enc_ft)\n",
        "            # returns the concatenated dataframe\n",
        "            full_df_one_hot_enc = concat_with_rest(df,one_hot_enc_df,self.one_hot_enc_ft)\n",
        "            return full_df_one_hot_enc\n",
        "        else:\n",
        "            print(\"One or more features are not in the dataframe\")\n",
        "            return df"
      ],
      "metadata": {
        "id": "azwZwuowFcoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OrdinalFeatNames(BaseEstimator,TransformerMixin):\n",
        "    def __init__(self,ordinal_enc_ft = ['Education level']):\n",
        "        self.ordinal_enc_ft = ordinal_enc_ft\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "    def transform(self,df):\n",
        "        if 'Education level' in df.columns:\n",
        "            ordinal_enc = OrdinalEncoder()\n",
        "            df[self.ordinal_enc_ft] = ordinal_enc.fit_transform(df[self.ordinal_enc_ft])\n",
        "            return df\n",
        "        else:\n",
        "            print(\"Education level is not in the dataframe\")\n",
        "            return df"
      ],
      "metadata": {
        "id": "xIdLmPIrFpIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinMaxWithFeatNames(BaseEstimator,TransformerMixin):\n",
        "    def __init__(self,min_max_scaler_ft = ['Age', 'Income', 'Employment length']):\n",
        "        self.min_max_scaler_ft = min_max_scaler_ft\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "    def transform(self,df):\n",
        "        if (set(self.min_max_scaler_ft).issubset(df.columns)):\n",
        "            min_max_enc = MinMaxScaler()\n",
        "            df[self.min_max_scaler_ft] = min_max_enc.fit_transform(df[self.min_max_scaler_ft])\n",
        "            return df\n",
        "        else:\n",
        "            print(\"One or more features are not in the dataframe\")\n",
        "            return df"
      ],
      "metadata": {
        "id": "QZYnavvSFpEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChangeToNumTarget(BaseEstimator,TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "    def transform(self,df):\n",
        "        if 'Is high risk' in df.columns:\n",
        "            df['Is high risk'] = pd.to_numeric(df['Is high risk'])\n",
        "            return df\n",
        "        else:\n",
        "            print(\"Is high risk is not in the dataframe\")\n",
        "            return df"
      ],
      "metadata": {
        "id": "5sor9U5rFpCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Oversample(BaseEstimator,TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit(self,df):\n",
        "        return self\n",
        "    def transform(self,df):\n",
        "        if 'Is high risk' in df.columns:\n",
        "            # smote function to oversample the minority class to fix the imbalance data\n",
        "            oversample = SMOTE(sampling_strategy='minority')\n",
        "            X_bal, y_bal = oversample.fit_resample(df.loc[:, df.columns != 'Is high risk'],df['Is high risk'])\n",
        "            df_bal = pd.concat([pd.DataFrame(X_bal),pd.DataFrame(y_bal)],axis=1)\n",
        "            return df_bal\n",
        "        else:\n",
        "            print(\"Is high risk is not in the dataframe\")\n",
        "            return df"
      ],
      "metadata": {
        "id": "NMBHmkhwFpAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_pipeline(df):\n",
        "    # Create the pipeline that will call all the class from OutlierRemoval to OversampleSMOTE in one go\n",
        "    pipeline = Pipeline([\n",
        "        ('outlier_remover', OutlierRemover()),\n",
        "        ('feature_dropper', DropFeatures()),\n",
        "        ('time_conversion_handler', TimeConversionHandler()),\n",
        "        ('retiree_handler', RetireeHandler()),\n",
        "        ('skewness_handler', SkewnessHandler()),\n",
        "        ('binning_num_to_yn', BinningNumToYN()),\n",
        "        ('one_hot_with_feat_names', OneHotWithFeatNames()),\n",
        "        ('ordinal_feat_names', OrdinalFeatNames()),\n",
        "        ('min_max_with_feat_names', MinMaxWithFeatNames()),\n",
        "        ('change_to_num_target', ChangeToNumTarget()),\n",
        "        ('oversample', Oversample())\n",
        "    ])\n",
        "    df_pipe_prep = pipeline.fit_transform(df)\n",
        "    return df_pipe_prep"
      ],
      "metadata": {
        "id": "ljBn9dtAFo-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.mode.chained_assignment = None  # Hide the copy warning\n",
        "cc_train_prep = full_pipeline(cc_train_copy)\n",
        "cc_train_prep.shape"
      ],
      "metadata": {
        "id": "gpCcDJaFFo8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "cc_train_prep.head()"
      ],
      "metadata": {
        "id": "iNo8mu1zF0mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cc_train_prep, y_cc_train_prep = cc_train_prep.loc[:, cc_train_prep.columns != 'Is high risk'], cc_train_prep['Is high risk'].astype('int64')\n"
      ],
      "metadata": {
        "id": "CCz99Hl5F0iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Classifiers dictionary\n",
        "classifiers = {\n",
        "    'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'support_vector_machine': SVC(probability=True, random_state=42),\n",
        "    'decision_tree': DecisionTreeClassifier(random_state=42),\n",
        "    'random_forest': RandomForestClassifier(random_state=42),\n",
        "    'k_nearest_neighbors': KNeighborsClassifier(),\n",
        "    'gradient_boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'neural_network': MLPClassifier(random_state=42, max_iter=1000),\n",
        "    'adaboost': AdaBoostClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6lXL-jk_F0gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the feature importance of the classifier, and plot it\n",
        "def feat_importance_plot(model_trn, model_name):\n",
        "    if model_name not in ['sgd','support_vector_machine','gaussian_naive_bayes','k_nearest_neighbors','bagging','neural_network']:\n",
        "        # change xtick font size\n",
        "        plt.rcParams['xtick.labelsize'] = 12\n",
        "        plt.rcParams['ytick.labelsize'] = 12\n",
        "        # top 10 most predictive features\n",
        "        top_10_feat = FeatureImportances(model_trn, relative=False, topn=10)\n",
        "        # top 10 least predictive features\n",
        "        bottom_10_feat = FeatureImportances(model_trn, relative=False, topn=-10)\n",
        "        #change the figure size\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        #change x label font size\n",
        "        plt.xlabel('xlabel', fontsize=14)\n",
        "        # Fit to get the feature importances\n",
        "        top_10_feat.fit(X_cc_train_prep, y_cc_train_prep)\n",
        "        # show the plot\n",
        "        top_10_feat.show()\n",
        "        print('\\n')\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.xlabel('xlabel', fontsize=14)\n",
        "        # Fit to get the feature importances\n",
        "        bottom_10_feat.fit(X_cc_train_prep, y_cc_train_prep)\n",
        "        # show the plot\n",
        "        bottom_10_feat.show()\n",
        "        print('\\n')\n",
        "    else:\n",
        "        print('No feature importance for {0}'.format(model_name))\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "fl6IkXa9F0eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the y prediction\n",
        "def y_prediction_func(model_trn,model_name,final_model=False):\n",
        "    if final_model == False:\n",
        "        # check if y_train_copy_pred exists, if not create it\n",
        "        y_cc_train_pred_path = Path('saved_models/{0}/y_train_copy_pred_{0}.sav'.format(model_name))\n",
        "        try:\n",
        "            y_cc_train_pred_path.resolve(strict=True)\n",
        "        except FileNotFoundError:\n",
        "            #cross validation prediction with kfold = 10\n",
        "            y_cc_train_pred = cross_val_predict(model_trn,X_cc_train_prep,y_cc_train_prep,cv=10,n_jobs=-1)\n",
        "            #save the predictions\n",
        "            joblib.dump(y_cc_train_pred,y_cc_train_pred_path)\n",
        "            return y_cc_train_pred\n",
        "        else:\n",
        "            # if it exist load the predictions\n",
        "            y_cc_train_pred = joblib.load(y_cc_train_pred_path)\n",
        "            return y_cc_train_pred\n",
        "    else:\n",
        "        # check if y_train_copy_pred exists, if not create it\n",
        "        y_cc_train_pred_path_final = Path('saved_models_final/{0}/y_train_copy_pred_{0}_final.sav'.format(model_name))\n",
        "        try:\n",
        "            y_cc_train_pred_path_final.resolve(strict=True)\n",
        "        except FileNotFoundError:\n",
        "            #cross validation prediction with kfold = 10\n",
        "            y_cc_train_pred_final = cross_val_predict(model_trn,X_cc_train_prep,y_cc_train_prep,cv=10,n_jobs=-1)\n",
        "            #save the predictions\n",
        "            joblib.dump(y_cc_train_pred_final,y_cc_train_pred_path_final)\n",
        "            return y_cc_train_pred_final\n",
        "        else:\n",
        "            # if it exist load the predictions\n",
        "            y_cc_train_pred_final = joblib.load(y_cc_train_pred_path_final)\n",
        "            return y_cc_train_pred_final"
      ],
      "metadata": {
        "id": "9rnc-5FdF6p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot the confusion matrix\n",
        "def confusion_matrix_func(model_trn,model_name,final_model=False):\n",
        "    if final_model == False:\n",
        "        fig, ax = plt.subplots(figsize=(8,8))\n",
        "        #plot confusion matrix\n",
        "        conf_matrix = ConfusionMatrixDisplay.from_predictions(y_cc_train_prep,y_prediction_func(model_trn,model_name),ax=ax, cmap='Blues',values_format='d')\n",
        "        # remove the grid\n",
        "        plt.grid(visible=None)\n",
        "        # increase the font size of the x and y labels\n",
        "        plt.xlabel('Predicted label', fontsize=14)\n",
        "        plt.ylabel('True label', fontsize=14)\n",
        "        #give a title to the plot using the model name\n",
        "        plt.title('Confusion Matrix', fontsize=14)\n",
        "        #show the plot\n",
        "        plt.show()\n",
        "        print('\\n')\n",
        "    else:\n",
        "        fig, ax = plt.subplots(figsize=(8,8))\n",
        "        #plot confusion matrix\n",
        "        conf_matrix_final = ConfusionMatrixDisplay.from_predictions(y_cc_train_prep,y_prediction_func(model_trn,model_name,final_model=True),ax=ax, cmap='Blues',values_format='d')\n",
        "        # remove the grid\n",
        "        plt.grid(visible=None)\n",
        "        # increase the font size of the x and y labels\n",
        "        plt.xlabel('Predicted label', fontsize=14)\n",
        "        plt.ylabel('True label', fontsize=14)\n",
        "        #give a title to the plot using the model name\n",
        "        plt.title('Confusion Matrix', fontsize=14)\n",
        "        #show the plot\n",
        "        plt.show()\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "qeiBJs0CF6ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot the roc curve\n",
        "def roc_curve_func(model_trn,model_name,final_model=False):\n",
        "    if final_model == False:\n",
        "        # check if y probabilities file exists, if not create it\n",
        "        y_proba_path = Path('saved_models/{0}/y_cc_train_proba_{0}.sav'.format(model_name))\n",
        "        try:\n",
        "            y_proba_path.resolve(strict=True)\n",
        "        except FileNotFoundError:\n",
        "            y_cc_train_proba = model_trn.predict_proba(X_cc_train_prep)\n",
        "            joblib.dump(y_cc_train_proba,y_proba_path)\n",
        "        else:\n",
        "            # if path exist load the y probabilities file\n",
        "            y_cc_train_proba = joblib.load(y_proba_path)\n",
        "        skplt.metrics.plot_roc(y_cc_train_prep, y_cc_train_proba, title = 'ROC curve for {0}'.format(model_name), cmap='cool',figsize=(8,6), text_fontsize='large')\n",
        "        #remove the grid\n",
        "        plt.grid(visible=None)\n",
        "        plt.show()\n",
        "        print('\\n')\n",
        "    else:\n",
        "        # check if y probabilities file exists, if not create it\n",
        "        y_proba_path_final = Path('saved_models_final/{0}/y_cc_train_proba_{0}_final.sav'.format(model_name))\n",
        "        try:\n",
        "            y_proba_path_final.resolve(strict=True)\n",
        "        except FileNotFoundError:\n",
        "            y_cc_train_proba_final = model_trn.predict_proba(X_cc_train_prep)\n",
        "            joblib.dump(y_cc_train_proba_final,y_proba_path_final)\n",
        "        else:\n",
        "            # if path exist load the y probabilities file\n",
        "            y_cc_train_proba_final = joblib.load(y_proba_path_final)\n",
        "        skplt.metrics.plot_roc(y_cc_train_prep, y_cc_train_proba_final, title = 'ROC curve for {0}'.format(model_name), cmap='cool',figsize=(8,6), text_fontsize='large')\n",
        "        #remove the grid\n",
        "        plt.grid(visible=None)\n",
        "        plt.show()\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "qGt1tLOpF6kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display the classification report\n",
        "def score_func(model_trn, model_name, final_model=False):\n",
        "    if final_model == False:\n",
        "        class_report = classification_report(y_cc_train_prep,y_prediction_func(model_trn,model_name))\n",
        "        print(class_report)\n",
        "    else:\n",
        "        class_report_final = classification_report(y_cc_train_prep,y_prediction_func(model_trn,model_name,final_model=True))\n",
        "        print(class_report_final)"
      ],
      "metadata": {
        "id": "Da01NjRaF6h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model\n",
        "def train_model(model,model_name,final_model=False):\n",
        "    # if we are not training the final model\n",
        "    if final_model == False:\n",
        "        # check if the model file exist and if not create, train and save it\n",
        "        model_file_path = Path('saved_models/{0}/{0}_model.sav'.format(model_name))\n",
        "        try:\n",
        "            model_file_path.resolve(strict=True)\n",
        "        except FileNotFoundError:\n",
        "            if model_name == 'sgd':\n",
        "                # for sgd, loss = 'hinge' does not have a predict_proba method. Therefore, we use a calibrated model\n",
        "                calibrated_model = CalibratedClassifierCV(model, cv=10, method='sigmoid')\n",
        "                model_trn = calibrated_model.fit(X_cc_train_prep,y_cc_train_prep)\n",
        "            else:\n",
        "                model_trn = model.fit(X_cc_train_prep,y_cc_train_prep)\n",
        "            joblib.dump(model_trn,model_file_path)\n",
        "            # plot the most and least predictive features\n",
        "            return model_trn\n",
        "        else:\n",
        "            # if path exist load the model\n",
        "            model_trn = joblib.load(model_file_path)\n",
        "            # plot the most and least predictive features\n",
        "            return model_trn\n",
        "    else:\n",
        "        # check if the final model file exist and if not create, train and save it\n",
        "        final_model_file_path = Path('saved_models_final/{0}/{0}_model.sav'.format(model_name))\n",
        "        try:\n",
        "            final_model_file_path.resolve(strict=True)\n",
        "        except FileNotFoundError:\n",
        "            model_trn = model.fit(X_cc_train_prep,y_cc_train_prep)\n",
        "            joblib.dump(model_trn,final_model_file_path)\n",
        "            # plot the most and least predictive features\n",
        "            return model_trn\n",
        "        else:\n",
        "            # if path exist load the model\n",
        "            model_trn = joblib.load(final_model_file_path)\n",
        "            # plot the most and least predictive features\n",
        "            return model_trn"
      ],
      "metadata": {
        "id": "rZfyI0veGBeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def folder_check_model():\n",
        "    # check if the folder for saving the model exists, if not create it\n",
        "    if not os.path.exists('saved_models/{}'.format(model_name)):\n",
        "        os.makedirs('saved_models/{}'.format(model_name))"
      ],
      "metadata": {
        "id": "xxxhGFHuGBaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over all the models\n",
        "for model_name,model in classifiers.items():\n",
        "    # title formatting\n",
        "    print('\\n')\n",
        "    print('\\n')\n",
        "    print('  {}  '.center(50,'-').format(model_name))\n",
        "    print('\\n')\n",
        "    # check if the folder for saving the model exists, if not create it\n",
        "    folder_check_model()\n",
        "    # train the model\n",
        "    model_trn = train_model(model,model_name)\n",
        "    # print the scores from the classification report\n",
        "    score_func(model_trn, model_name)\n",
        "    # plot the ROC curve\n",
        "    roc_curve_func(model_trn,model_name)\n",
        "    # plot the confusion matrix\n",
        "    confusion_matrix_func(model_trn,model_name)\n",
        "    # plot feature importance\n",
        "    feat_importance_plot(model_trn, model_name)\n",
        "    warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "K3UyJxTZGBYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_test_copy.head(5)"
      ],
      "metadata": {
        "id": "h6BLQEqsGBWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_test_prep = full_pipeline(cc_test_copy)"
      ],
      "metadata": {
        "id": "1DpRxEobGBTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the train data into X and y (target)\n",
        "X_cc_test_prep, y_cc_test_prep = cc_test_prep.loc[:, cc_test_prep.columns != 'Is high risk'], cc_test_prep['Is high risk'].astype('int64')"
      ],
      "metadata": {
        "id": "vb_LFHMHGT29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "model_trn = train_model(classifiers['gradient_boosting'],'gradient_boosting')\n",
        "final_predictions = model_trn.predict(X_cc_test_prep)\n",
        "final_predictions.shape"
      ],
      "metadata": {
        "id": "wNOAgQG4GTzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_correct = sum(final_predictions == y_cc_test_prep)\n",
        "print(n_correct/len(final_predictions))"
      ],
      "metadata": {
        "id": "KQqDBoJ6GTxo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}